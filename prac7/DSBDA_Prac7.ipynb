{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1287af74-ed25-420b-bdc5-152c88dcb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b64c96-6c85-4965-a2e9-c756e4bbeadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Hello Everyone. Welcome to the Python Programming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df83deb-52c5-404f-9809-20278ac5061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f5d169-d901-4866-b95d-2d03b078dfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      "Hello Everyone. Welcome to the Python Programming\n",
      "\n",
      "Sentence Tokenization:\n",
      "['Hello Everyone.', 'Welcome to the Python Programming']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Document:\")\n",
    "print(document)\n",
    "print(\"\\nSentence Tokenization:\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a260491b-3730-4cfd-a6ee-c109c55d3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca07f1c-5c27-45ea-8083-57e0221e96f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', 'Everyone', '.'], ['Welcome', 'to', 'the', 'Python', 'Programming']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea257859-e8f7-42f9-aeae-fee013009d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = [pos_tag(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983df32b-edd4-4c0e-a302-ed213234d2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Hello', 'NNP'), ('Everyone', 'NNP'), ('.', '.')], [('Welcome', 'VB'), ('to', 'TO'), ('the', 'DT'), ('Python', 'NNP'), ('Programming', 'NNP')]]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a84e5f02-e343-48f8-993e-cc0efab334bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [[word for word in sentence if word.lower() not in stop_words] for sentence in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ae0c62b-64b1-4b9d-bac2-050916923409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Everyone. Welcome to the Python Programming\n",
      "[['Hello', 'Everyone', '.'], ['Welcome', 'Python', 'Programming']]\n"
     ]
    }
   ],
   "source": [
    "print(document)\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e9b328-ce81-407b-a8a9-47632d699f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [[stemmer.stem(word) for word in sentence] for sentence in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd6aa954-44b7-4fd8-b69a-5e415a383c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Everyone. Welcome to the Python Programming\n",
      "[['hello', 'everyon', '.'], ['welcom', 'to', 'the', 'python', 'program']]\n"
     ]
    }
   ],
   "source": [
    "print(document)\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33371214-c18f-4e30-97ce-9baa7e6fece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e03875bf-c69c-4a2d-8128-0eedaa9dc74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Everyone. Welcome to the Python Programming\n",
      "[['Hello', 'Everyone', '.'], ['Welcome', 'to', 'the', 'Python', 'Programming']]\n"
     ]
    }
   ],
   "source": [
    "print(document)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0ce85cf-5f16-44f3-880a-d1da1e0249a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786b519e-9e6e-4a69-a732-3ac0dcb3f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'The quick brown fox jumps over the lazy dog',\n",
    "    'The brown fox is quick',\n",
    "    'The lazy dog is sleeping'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75cd1163-aee9-44b1-acbe-83dd19dabdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus):\n",
    "    tokenized_docs = [doc.lower().split() for doc in corpus]\n",
    "    tf_docs = [Counter(tokens) for tokens in tokenized_docs]\n",
    "    n_docs = len(corpus)\n",
    "    idf = {token: math.log(n_docs / sum(1 for doc in tokenized_docs if token in doc)) for doc in tokenized_docs for token in set(doc)}\n",
    "    return [{token: tf_docs[i][token] * idf[token] for token in tf_docs[i]} for i in range(n_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d62a6cd-6b38-4a58-85a5-6f1f2cb9668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs = tf_idf(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e9773d7-83c8-47e4-bf4f-0b268101251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: {'the': 0.0, 'quick': 0.4054651081081644, 'brown': 0.4054651081081644, 'fox': 0.4054651081081644, 'jumps': 1.0986122886681098, 'over': 1.0986122886681098, 'lazy': 0.4054651081081644, 'dog': 0.4054651081081644}\n",
      "Document 2: {'the': 0.0, 'brown': 0.4054651081081644, 'fox': 0.4054651081081644, 'is': 0.4054651081081644, 'quick': 0.4054651081081644}\n",
      "Document 3: {'the': 0.0, 'lazy': 0.4054651081081644, 'dog': 0.4054651081081644, 'is': 0.4054651081081644, 'sleeping': 1.0986122886681098}\n"
     ]
    }
   ],
   "source": [
    "for i, tfidf_doc in enumerate(tfidf_docs):\n",
    "    print(f\"Document {i+1}: {tfidf_doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1dfa3b-cea4-48b9-99c7-43496db4652b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4d3fc-86f8-4450-88ba-f6878df2aa10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
